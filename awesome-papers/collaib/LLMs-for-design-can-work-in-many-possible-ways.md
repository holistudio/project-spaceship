[[design-as-language-differs-from-verbal-language]] so LLMs like ChatGPT can work with designers in many possible ways.

[[Hypar with OpenAI API]]
"Oana Taut: My (quasi historical ðŸ˜…) thought process:

-humans develop speech and use it to communicate ideas

-humans abstract ideas with drawings and use it to pass knowledge through generations. Some humans are better at it than others. Architecture is built around this diferenciating ability

-architects create rules to further abstract concepts and define the drawings of buildings

-tech creates the ability to abstract the rules and create more precise and complex drawings and buildings

(Each step before evolution meant: abstraction enables to capture more breath of information)

-AI becomes more mainstream and learns speech

â€¦.

Is it the logical next step to teach it to draw and forget all the abstraction and rules we already built specialisation around?

Will future architects draw space like an essay? Leave dimensions, proportions, fine tuning, not to mention analysis that informs space, to AI (stochastic initialisation) [[AI-helps-design-convergence]]"

"AH: I love this comment. I don't think a technology like this means we have to abandon the tools already at our disposal â€” I don't even think that's possible. I imagine the ideal future design scenario being highly multi-modal â€” AI-powered with high-level text descriptions (and loose sketch inputs too!) as well as manual precision data entry, modeling, explicit commands + operations, all working together."

[[DALL-E]]
"DALL-E is a 12-billion parameter version of GPT-3 trained to generate images from text descriptions, using a dataset of textâ€“image pairs. Weâ€™ve found that it has a diverse set of capabilities, including creating anthropomorphized versions of animals and objects, combining unrelated concepts in plausible ways, rendering text, and applying transformations to existing images."