**Source:** https://openai.com/blog/dall-e/

**Authors:** [[OpenAI]]

**Quotes:** 

DALL-E is a 12-billion parameter version of GPT-3 trained to generate images from text descriptions, using a dataset of text–image pairs. We’ve found that it has a diverse set of capabilities, including creating anthropomorphized versions of animals and objects, combining unrelated concepts in plausible ways, rendering text, and applying transformations to existing images.

**Notes:**

A first step of applying this more to architect's is to train this AI on additional pairings of architectural drawings and programmatic descriptions. Latent variables can be trained to automatically recognize early concept sketches vs more finished schematics.

The major limitation of this model is that the images are generated from text, and that's that. While artists can certainly take inspiration from the generated image and re-work them, the AI never learns from further iterations that artists make and therefore the AI is never seen as a creative partner, only a one-shot tool.
